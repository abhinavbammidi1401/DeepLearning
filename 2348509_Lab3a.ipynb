{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent without Momentum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0 f([-10.8968907]) = 486.27783\n",
      ">1 f([-92.21534625]) = 7118.29543\n",
      ">2 f([-1080.69092796]) = 616467.16873\n",
      ">3 f([-65969.48058451]) = 2177965368.71240\n",
      ">4 f([-2.18060415e+08]) = 23775178791786300.00000\n",
      "Done!\n",
      "f(-218060414.7935785) = 23775178791786300.000000\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "from numpy.random import rand, seed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# objective function\n",
    "def objective(w):\n",
    "    return 0.5 * w**2 - 30 * w + 100\n",
    "\n",
    "# derivative of objective function\n",
    "def derivative(w):\n",
    "    return w**2/2 - 60*w + 100\n",
    "\n",
    "# gradient descent algorithm\n",
    "def gradient_descent(objective, derivative, bounds, n_iter, step_size):\n",
    "    # generate an initial point\n",
    "    solution = bounds[:, 0] + rand(len(bounds)) * (bounds[:,1] - bounds[:, 0])\n",
    "    for i in range(n_iter):# run the gradient descent\n",
    "        gradient = derivative(solution) # calculate gradient\n",
    "        solution = solution - step_size * gradient # take a step\n",
    "        solution_eval = objective(solution) # evaluate condidate point\n",
    "        print('>%d f(%s) = %.5f'% (i, solution, solution_eval))\n",
    "        if abs(solution_eval) == float('inf'):\n",
    "            break\n",
    "    return [solution, solution_eval]\n",
    "    \n",
    "seed(2) # seed the pseudo random number generator\n",
    "bounds = asarray([[-1.0, 1.0]]) # define the range for input\n",
    "n_iter = 5 # define total number of iterations\n",
    "step_size = 0.1 # define the step size\n",
    "best, score = gradient_descent(objective, derivative, bounds, n_iter, step_size)\n",
    "print('Done!')\n",
    "print('f(%s) = %f' %(best[0], score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent with Momentum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0 f([-10.8968907]) = 486.27783\n",
      ">1 f([-95.4460104]) = 7518.35076\n",
      ">2 f([-1158.98385381]) = 706491.30231\n",
      ">3 f([-75604.1269991]) = 2860260233.45773\n",
      ">4 f([-2.86350773e+08]) = 40998391302971216.00000\n",
      ">5 f([-4.09984036e+15]) = 8404345495231677834197060288512.00000\n",
      ">6 f([-8.4043455e+29]) = 353165116016119820683115243624986846997078019470827764842496.00000\n",
      ">7 f([-3.53165116e+58]) = 623627995853396967667176684143402473774086222930762814155572757133220878817528842268645795221692914143302798761000960.00000\n",
      ">8 f([-6.23627996e+115]) = 1944559386060622674623852948501672005143577034154054895535600614909709703402986011143838822372781151139179904090168633741115989981778992510379075553312945219406481453438488727281957084627383549516889750307678911619345949595128561664.00000\n",
      ">9 f([-1.94455939e+230]) = inf\n",
      "Done!\n",
      "f([-1.94455939e+230]) + inf\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "from numpy.random import rand, seed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# objective function\n",
    "def objective(w):\n",
    "    return 0.5 * w**2 - 30 * w + 100\n",
    "\n",
    "# derivative of objective function\n",
    "def derivative(w):\n",
    "    return w**2/2 - 60*w + 100\n",
    "\n",
    "# gradient descent algorithm\n",
    "def gradient_descent(objective, derivative, bounds, n_iter, step_size, momentum):\n",
    "    solution = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n",
    "    change = 0.0 # keep track of the change\n",
    "    for i in range(n_iter):\n",
    "        gradient = derivative(solution) # calculate gradient\n",
    "        new_change = step_size * gradient + momentum * change # caclculate update\n",
    "        solution = solution - new_change # take a step\n",
    "        change = new_change # save the change\n",
    "        solution_eval = objective(solution) # evaluate candidate point\n",
    "        print('>%d f(%s) = %.5f' %(i, solution, solution_eval))\n",
    "        if abs(solution_eval) == float('inf'):\n",
    "            break\n",
    "    return [solution, solution_eval]\n",
    "    \n",
    "seed(2) # seed the pseudo random number generator\n",
    "bounds = asarray([[-1.0, 1.0]]) # define the range for input\n",
    "n_iter = 10 # define total number of iterations\n",
    "step_size = 0.1 # define the step size\n",
    "momentum = 0.3 # define momentum\n",
    "\n",
    "# perform gradient descent\n",
    "best, score = gradient_descent(objective, derivative, bounds, n_iter, step_size, momentum)\n",
    "print('Done!')\n",
    "print('f(%s) + %f' %(best, score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
